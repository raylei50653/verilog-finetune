import os
import glob
import shutil
import torch
import gc

# 1. FORCE USE OF STANDARD HOME CACHE
# Since we verified that ~/.cache/huggingface has 21GB of data, we should use it.
# This avoids partial/incomplete local caches triggering re-downloads.
active_cache = os.path.expanduser("~/.cache/huggingface")
print(f"ğŸ“¦ Using STANDARD HOME cache: {active_cache}")

os.environ["HF_HOME"] = active_cache
os.environ["HUGGINGFACE_HUB_CACHE"] = os.path.join(active_cache, "hub")

# Now import ML libraries
from unsloth import FastLanguageModel

def get_latest_checkpoint(base_dir):
    checkpoints = glob.glob(os.path.join(base_dir, "checkpoint-*"))
    final_adapter = os.path.join(base_dir, "final_adapter")
    
    if os.path.exists(final_adapter):
        return final_adapter
    if not checkpoints:
        return None
    checkpoints.sort(key=lambda x: int(x.split("-")[-1]))
    return checkpoints[-1]

def convert_adapter_to_gguf(adapter_path, output_base_name):
    if not adapter_path or not os.path.exists(adapter_path):
        print(f"âŒ Skip: Path {adapter_path} not found.")
        return

    print(f"\nğŸš€ Found Adapter: {adapter_path}")
    
    # æœ€çµ‚è¼¸å‡ºç›®éŒ„
    gguf_models_dir = "gguf_models"
    os.makedirs(gguf_models_dir, exist_ok=True)

    # æš«å­˜å·¥ä½œç›®éŒ„ (çµ•å°è·¯å¾‘)
    temp_base_dir = os.path.abspath("temp_gguf_process")

    # 1. å®šç¾© FP16 æ°¸ä¹…å­˜æ”¾è·¯å¾‘ (åœ¨ HF cache å…§)
    fp16_cache_dir = os.path.join(active_cache, "merged_models", output_base_name + "-fp16")
    
    is_merged_exists = os.path.exists(os.path.join(fp16_cache_dir, "config.json"))

    try:
        if not is_merged_exists:
            print(f"ğŸ”„ [Step 1] Merging Adapter to Base (First time only)...")
            os.makedirs(fp16_cache_dir, exist_ok=True)
            # åªæœ‰ä¸å­˜åœ¨æ™‚æ‰è¼‰å…¥ Adapter ä¸¦åˆä½µ
            model, tokenizer = FastLanguageModel.from_pretrained(
                model_name = adapter_path,
                max_seq_length = 2048,
                dtype = None,
                load_in_4bit = True, # æ”¹å› True ä»¥é©æ‡‰ 12GB VRAM
            )
            print(f"ğŸ’¾ Saving merged FP16 to cache: {fp16_cache_dir}")
            model.save_pretrained_merged(fp16_cache_dir, tokenizer, save_method = "merged_16bit")
            print("âœ… FP16 model saved.")
            
            # é‡‹æ”¾è¨˜æ†¶é«”ï¼Œç¢ºä¿å¾ŒçºŒæ­¥é©Ÿæœ‰ä¹¾æ·¨ç’°å¢ƒ
            del model
            del tokenizer
            gc.collect()
            torch.cuda.empty_cache()
        else:
            print(f"â© [Step 1] Found existing FP16 in cache. Skipping merge & write.")

    except Exception as e:
        print(f"âŒ Failed in Step 1: {e}")
        return

    # å®šç¾©æˆ‘å€‘éœ€è¦çš„é‡åŒ–ç‰ˆæœ¬
    target_quants = [
        ("q4_k_m", "Q4_K_M"),
        ("q3_k_m", "Q3_K_M")
    ]

    try:
        for q_method, suffix in target_quants:
            output_filename = f"{output_base_name}.{suffix}.gguf"
            final_path = os.path.join(gguf_models_dir, output_filename)
            
            if os.path.exists(final_path):
                print(f"â© Skip: {final_path} already exists.")
                continue

            print(f"\nâš™ï¸  Processing {suffix} -> {final_path}")
            
            # ç‚ºæ¯å€‹é‡åŒ–ä»»å‹™å»ºç«‹ä¹¾æ·¨çš„å­ç›®éŒ„
            current_temp_dir = os.path.join(temp_base_dir, q_method)
            if os.path.exists(current_temp_dir):
                shutil.rmtree(current_temp_dir)
            os.makedirs(current_temp_dir)
            
            try:
                # æ¯æ¬¡è½‰æ›å‰é‡æ–°è¼‰å…¥æ¨¡å‹ï¼Œç¢ºä¿ç‹€æ…‹ä¹¾æ·¨
                # å›é€€åˆ°ä½¿ç”¨ adapter_pathï¼Œå› ç‚ºå¾ merged_fp16 + load_in_4bit é€²è¡Œ GGUF è½‰æ›ä¸ç©©å®š
                print(f"ğŸ”„ Loading adapter for {q_method}...")
                model, tokenizer = FastLanguageModel.from_pretrained(
                    model_name = adapter_path,
                    max_seq_length = 2048,
                    dtype = None,
                    load_in_4bit = True,
                )

                # ä½¿ç”¨ Unsloth å…§å»ºè½‰æ›
                model.save_pretrained_gguf(
                    current_temp_dir, 
                    tokenizer, 
                    quantization_method = q_method
                )
                
                # å°‹æ‰¾ä¸¦ç§»å‹•æª”æ¡ˆ
                found = False
                for f in os.listdir(current_temp_dir):
                    if f.endswith(".gguf"):
                        src = os.path.join(current_temp_dir, f)
                        shutil.move(src, final_path)
                        print(f"âœ… Saved: {final_path}")
                        found = True
                        break
                
                if not found:
                    print(f"âŒ Error: No GGUF generated for {q_method}")

            except Exception as e:
                print(f"âŒ Failed processing {q_method}: {e}")
            
            finally:
                # æ¯å€‹è¿´åœˆçµæŸéƒ½é‡‹æ”¾è¨˜æ†¶é«”
                if 'model' in locals(): del model
                if 'tokenizer' in locals(): del tokenizer
                gc.collect()
                torch.cuda.empty_cache()
            
    finally:
        # ã€é—œéµã€‘ç„¡è«–ç™¼ç”Ÿä»€éº¼äº‹ï¼Œæœ€å¾Œä¸€å®šå¼·åˆ¶åˆªé™¤æ•´å€‹æš«å­˜è³‡æ–™å¤¾
        # é€™æœƒæŠŠè£¡é¢ä»»ä½•è‡ªå‹•ç”¢ç”Ÿçš„ .cache éƒ½ä¸€ä½µæ¸…æ‰
        if os.path.exists(temp_base_dir):
            print("ğŸ§¹ Cleaning up temporary directories...")
            try:
                shutil.rmtree(temp_base_dir)
            except OSError as e:
                print(f"âš ï¸ Warning: Failed to fully clean temp dir: {e}")

    # æ¸…ç†è¨˜æ†¶é«”
    print("ğŸ§¹ Cleaning up memory...")
    if 'model' in locals(): del model
    if 'tokenizer' in locals(): del tokenizer
    gc.collect()
    torch.cuda.empty_cache()

if __name__ == "__main__":
    verilog_adapter = get_latest_checkpoint("outputs")
    if verilog_adapter:
        convert_adapter_to_gguf(verilog_adapter, "verilog-llama-3-8b")
    else:
        print("âš ï¸ No checkpoints found.")
